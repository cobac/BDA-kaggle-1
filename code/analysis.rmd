---
title: First Kaggle competition, first submission - Group 3
author: 
 - David Coba
 - Sophie Elting
 - Enrico Erler
output:
  html_document:
    toc: true
---

# Setup environment 

```{r}
library(tidyverse) # Metapackage with lots of helpful functions
library(tidytext)

list.files(path = "../input/")
```

```{r}
directory_content = list.files("../input/bda2021big5/youtube-personality", full.names = TRUE)
print(directory_content)
```

```{r}
# Path to the transcripts directory with transcript .txt files
path_to_transcripts <- directory_content[2] 

# .csv filenames (see output above)
AudioVisual_file <- directory_content[3]
Gender_file      <- directory_content[4]
Personality_file <- directory_content[5]
```

## Import the data

### Import transcripts

```{r}
transcript_files <- list.files(path_to_transcripts, full.names = TRUE) 

print(head(transcript_files))
```

```{r}
vlogId <- basename(transcript_files)
vlogId <- str_replace(vlogId, pattern = ".txt$", replacement = "")
head(vlogId)
```

```{r}
transcripts_df <- tibble(
  # vlogId connects each transcripts to a vlogger
  vlogId=vlogId,
  # Read the transcript text from all file and store as a string
  Text = map_chr(transcript_files, ~ paste(readLines(.x), collapse = "\\n")), 
  # `filename` keeps track of the specific video transcript
  filename = transcript_files)
```

```{r}
transcripts_df %>% 
  head(2)
```

### Import personality scores

```{r}
# Import the Personality scores
pers_df <- read_delim(Personality_file, delim=" ")
head(pers_df)
```

### Import gender

```{r}
gender_df <- read.delim(Gender_file, head=FALSE, sep=" ", skip = 1)
# Add column names
names(gender_df) = c("vlogId", "gender")
head(gender_df)
```

### Import audiovisual data

```{r}
audiovisual_df <- read.delim(AudioVisual_file, head = TRUE, sep = " ")
head(audiovisual_df)
```

# Feature extraction from the transcripts

## Word-based features

### Setup

- Tokenize transcripts into words.

```{r}
words_df <- transcripts_df %>% 
  unnest_tokens(output = word, input = Text, token = "words") %>% 
  select(-filename)
head(words_df)
```

- Load dictionary of stop words

```{r}
data(stop_words)
```

### Total number of words

```{r}
count_words <- words_df %>%
  count(vlogId) %>%
  rename(total_words = n)
```

### "I"/"We" relative frequency

```{r}
count_i <- words_df %>%
  filter(word == "i" |
           word == "i'm" |
           word == "me" |
           word == "my" |
           word == "mine" |
           word == "myself") %>%
  count(vlogId) %>% 
  rename(total_i = n)

count_we <- words_df %>%
  filter(word == "we" |
           word == "we're" |
           word == "us" |
           word == "our" |
           word == "ours" |
           word == "ourselves") %>%
  count(vlogId) %>% 
  rename(total_we = n)

i_we_df <- count_words %>% 
  full_join(count_i,  by = "vlogId") %>%
  full_join(count_we, by = "vlogId") %>%
  replace_na(list(total_i = 0, total_we = 0)) %>%
  mutate(freq_i = total_i / total_words,
         freq_we = total_we / total_words,) 

head(i_we_df)
i_we_df <- i_we_df %>% 
  select(vlogId, freq_i, freq_we)
```

### Average word length

```{r}
avg_word_length_df <- words_df %>% 
  mutate(word_length = nchar(word)) %>%
  group_by(vlogId) %>%
  summarise(avg_word_length = mean(word_length))

head(avg_word_length_df)
```

### Negations

```{r}
negation_url <- "https://www.grammarly.com/blog/negatives/"
negation_words <- readLines(negation_url)[63:89] %>%
  str_replace("<li>", "") %>% 
  str_replace("</li>", "") %>%
  tibble(word = .) %>%
  filter(!grepl(" ", word),
         !grepl("<", word),
         word != "") %>% 
  mutate(word = str_to_lower(word))
head(negation_words)
```

```{r}
negation_df <- words_df %>%
  filter(word %in% negation_words$word) %>%
  count(vlogId) %>%
  full_join(count_words, by = "vlogId") %>%
  replace_na(list(n = 0)) %>%
  mutate(negation_freq = n / total_words) %>%
  select(vlogId, negation_freq)

head(negation_df)
```

### Filler words

```{r}
filler_words <- c("like", "look", "yeah", "nah", "well", "so", "anyway",
                  "alright", "obviously", "actually", "basically", "literally",
                  "er", "eh", "ehh",
                  "oh",
                  "um", "uhm", "uh", "huh",
                  "hmm", "hm", "mmm" ) %>%
  tibble(word = .)

filler_df <- words_df %>%
  semi_join(filler_words, by = "word") %>%
  count(vlogId) %>% 
  full_join(count_words, by = "vlogId") %>%
  mutate(filler_freq = n / total_words) %>%
  replace_na(list(filler_freq = 0)) %>%
  select(vlogId, filler_freq)

head(filler_df)
```

### Swear words

```{r}
# Download dictionary of swear words
download.file("https://www.cs.cmu.edu/~biglou/resources/bad-words.txt", "swear.txt")

swear_words <- readLines("swear.txt") %>%
  tibble(word = .) %>% 
  # Select only lines with a single word
  filter(!grepl("\\W+", word), word != "") 

swear_df <- words_df %>%
  semi_join(swear_words, by = "word") %>%
  count(vlogId) %>%
  rename(swear_words = n) %>%
  # Join with gender to add vloggs without swear words
  right_join(gender_df, by = "vlogId") %>% 
  replace_na(list(swear_words = 0)) %>%
  select(vlogId, swear_words)

head(swear_df)
```

### Sentiment analysis

#### Overall sentiment average and variance

```{r}
# afinn_dict <- get_sentiments("afinn") => Fails when run non-interactively

load_afinn <- function() {
  if (!file.exists('afinn.txt'))
    download.file("http://www2.imm.dtu.dk/pubdb/edoc/imm6010.zip","afinn.zip")
  unzip("afinn.zip")
  afinn <- read.delim("AFINN/AFINN-111.txt", sep="\t", col.names = c("word","score"), stringsAsFactors = FALSE) %>% rename(value = score)
  return(afinn)
}

afinn_dict <- load_afinn()

sentiment_df <- words_df %>%
  inner_join(afinn_dict, by = "word") %>%
  anti_join(stop_words, by = "word") %>% 
  group_by(vlogId) %>%
  summarize(sentiment_avg = mean(value),
            sentiment_var = var(value))

head(sentiment_df)
```

#### NRC emotions analysis

```{r}
# nrc_dict <- get_sentiments("nrc") => FAILS

load_nrc <- function() {
  if (!file.exists('nrc.txt'))
    download.file("https://www.dropbox.com/s/yo5o476zk8j5ujg/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt?dl=1","nrc.txt")
  nrc = read.table("nrc.txt", col.names=c("word", "sentiment", "applies"), stringsAsFactors = FALSE)
  nrc %>% filter(applies == 1) %>% select(-applies)
}

nrc_dict <- load_nrc() %>% 
  # Filter out because we ussed afinn for valence 
  filter(!sentiment %in% c("positive", "negative"))

emotions_df <- words_df %>%
  right_join(nrc_dict, by = "word") %>%
  count(vlogId, sentiment) %>%
  left_join(count_words, by = "vlogId") %>%
  mutate(freq = n / total_words) %>%
  select(-c(n, total_words)) %>%
  pivot_wider(id_cols = vlogId, names_from = sentiment, values_from = freq) %>%
  replace_na(list(anger = 0,
                  anticipation = 0,
                  disgust = 0,
                  fear = 0,
                  joy = 0,
                  sadness = 0,
                  surprise = 0,
                  trust = 0))
# Delete last row with trailling NA
emotions_df <- emotions_df[-length(emotions_df$vlogId), ]
head(emotions_df)
```

## Sentence-based features

### Setup

- Tokenize transcripts into sentences.

```{r}
sentences_df <- transcripts_df %>%
    unnest_tokens(output = sentence, input = Text, token = 'sentences') %>%
    select(-filename)

head(sentences_df)
```

### Frequency of questions

- Feature originally implemented by Group 2

- Count the number of sentences per vlogId
```{r}
count_sentences <- sentences_df %>% 
  group_by(vlogId) %>%
  count() %>%
  rename(total_sentences = n)

head(count_sentences)
```

Count the number of questions per vlogId
``` {r}
count_questions <- sentences_df %>%  
    mutate(sentence_end = str_sub(sentence, start = -1)) %>%
    group_by(vlogId) %>%
    filter(sentence_end == "?") %>%
    count() %>%
  rename(total_questions = n)

head(count_questions)
```

Calculate relative frequency of questions
```{r}
questions_df <- count_sentences %>% 
  full_join(count_questions,  by = "vlogId") %>%
  replace_na(list(total_sentences = 0, total_questions = 0)) %>%
  mutate(freq_questions = total_questions / total_sentences) %>%
  select(-total_sentences,-total_questions)

head(questions_df)
```

### Average sentence length (in number of words)

```{r}
avg_sentence_length_df <- sentences_df %>% 
  mutate(word_count = str_count(sentence, pattern = "\\w+")) %>%
  group_by(vlogId) %>%
  summarise(average_words_per_sentence = mean(word_count))

head(avg_sentence_length_df)
```

### Ratio of interruptions 
- Feature originally implemented by Group 2
- Sentences interrupted by a short phrase
  - The transcripts use n-dashes ("Sentence -- phrase."), two n-dashes ("Sentence -- phrase -- sentence.") and m-dashes ("---").
  - We have decided that just counting the number of plain texts dashes ("-") is a good approximate with a distribution close to the real interruptions.

```{r}
interruptions_df <- transcripts_df %>%
  group_by(vlogId) %>%
  full_join(count_sentences) %>%
  mutate(ratio_interruptions = str_count(Text, pattern = "-") / total_sentences) %>%
  select(-c(filename,Text,total_sentences))

head(interruptions_df)
```

# Merge all features into `vlogger_df`

- This is the list of features included in our models

```{r}
vlogger_df <- left_join(
  gender_df, pers_df                , by = "vlogId") %>%
  left_join(audiovisual_df          , by = "vlogId") %>%
  left_join(count_words             , by = "vlogId") %>%
  left_join(i_we_df                 , by = "vlogId") %>%
  left_join(avg_word_length_df      , by = "vlogId") %>%
  left_join(negation_df             , by = "vlogId") %>%
  left_join(sentiment_df            , by = "vlogId") %>% 
  left_join(filler_df               , by = "vlogId") %>% 
  left_join(swear_df                , by = "vlogId") %>% 
  left_join(emotions_df             , by = "vlogId") %>%
  left_join(questions_df            , by = "vlogId") %>%
  left_join(avg_sentence_length_df  , by = "vlogId") %>%
  left_join(interruptions_df               , by = "vlogId") 
  
head(vlogger_df)
```

# Predictive model

- For now we are just using all features without interactions to predict all personality dimensions
- We plan on using a different approach for the final submission

```{r}
modeling_df <- vlogger_df %>%
  select(-vlogId)
model <- lm(cbind(Extr, Agr, Cons, Emot, Open) ~ ., data = modeling_df)
```

- Print R-squared
```{r}
sapply(summary(model), function(x) x$r.squared)
```

- Print significant variables
```{r}
getSigFeatures <- function(summ, alpha) {
  tibble(as.data.frame(summ$coefficients)) %>%
    mutate(feature = rownames(summ$coefficients)) %>% 
    filter(`Pr(>|t|)` < alpha) %>%
    select(feature)
}
sapply(summary(model), function(x) getSigFeatures(x, 0.1))
```

# Generating predictions

## The test set

```{r}
testset <- vlogger_df %>% 
  filter(is.na(Extr))
head(testset)
```

## Predictions

```{r}
predictions <- predict(model, newdata = testset)
head(predictions)
```


```{r}
# Compute output data frame
output <- testset %>% 
  mutate(
    Extr = predictions[,'Extr'], 
    Agr  = predictions[,'Agr' ],
    Cons = predictions[,'Cons'],
    Emot = predictions[,'Emot'],
    Open = predictions[,'Open']
  ) %>%
  select(vlogId, Extr:Open) %>% 
  # Convert to the specified output format
  gather(pers_axis, Expected, -vlogId) %>%
  arrange(vlogId, pers_axis) %>% 
  unite(Id, vlogId, pers_axis) %>%
  write_csv(file = "predictions.csv") 

head(output)
```

# Division of labor

For the submission Enrico and Sophie implemented the sentiment analysis of the transcripts and Coba coded the rest of the features. The three of used worked together on a videconference to clean up the code and generate the first set of predictions.

For the final submission Enrico was responsible for gathering interesting features from other groups. Sophia coded the showcase of different models and improved the state of the descriptive text of the report. Coba implemented the step-wise feature selection for the final model. The three of use worked together on a videoconference to clean up the final report.


