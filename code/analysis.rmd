---
title: First Kaggle competition, first submission - Group 3
author: 
 - David Coba
 - Sophie Elting
 - Enrico Erler
output: html_document
---

# Setup environment 

```{r}

library(tidyverse) # Metapackage with lots of helpful functions
library(tidytext)

list.files(path = "../input/")
```

```{r}
directory_content = list.files("../input/bda2021big5/youtube-personality", full.names = TRUE)
print(directory_content)
```

```{r}
# Path to the transcripts directory with transcript .txt files
path_to_transcripts <- directory_content[2] 

# .csv filenames (see output above)
AudioVisual_file <- directory_content[3]
Gender_file      <- directory_content[4]
Personality_file <- directory_content[5]
```

# Import the data

## Import transcripts

```{r}
transcript_files <- list.files(path_to_transcripts, full.names = TRUE) 

print(head(transcript_files))
```

```{r}
vlogId <- basename(transcript_files)
vlogId <- str_replace(vlogId, pattern = ".txt$", replacement = "")
head(vlogId)
```

```{r}
transcripts_df <- tibble(
    
  # vlogId connects each transcripts to a vlogger
  vlogId=vlogId,
  
  # Read the transcript text from all file and store as a string
  Text = map_chr(transcript_files, ~ paste(readLines(.x), collapse = "\\n")), 
  
  # `filename` keeps track of the specific video transcript
  filename = transcript_files
)
```

```{r}
transcripts_df %>% 
  head(2)
```

## Import personality scores

```{r}
# Import the Personality scores
pers_df <- read_delim(Personality_file, delim=" ")
head(pers_df)
```

## Import gender

```{r}
gender_df <- read.delim(Gender_file, head=FALSE, sep=" ", skip = 1)
# Add column names
names(gender_df) = c('vlogId', 'gender')
head(gender_df)
```

## Import audiovisual data
```{r}
audiovisual_df <- read.delim(AudioVisual_file, head = TRUE, sep = " ")
head(audiovisual_df)
```

# Feature extraction from transcript texts

- Tokenize into words

```{r}
words_df <- transcripts_df %>% 
  unnest_tokens(word, Text, token = 'words') %>% 
  select(-filename)
head(words_df)
```

- Load dictionary of stop words
```{r}
data(stop_words)
```

## Helper variables
- Total number of words
```{r}
count_words <- words_df %>%
  count(vlogId) %>%
  rename(total_words = n)
```

## "I"/"We" relative frequency

```{r}
count_i <- words_df %>%
  filter(word == "i") %>%
  count(vlogId) %>% 
  rename(total_i = n)

count_we <- words_df %>%
  filter(word == "we") %>%
  count(vlogId) %>% 
  rename(total_we = n)

i_we_df <- count_words %>% 
  full_join(count_i,  by = "vlogId") %>%
  full_join(count_we, by = "vlogId") %>%
  replace_na(list(total_i = 0, total_we = 0)) %>%
  mutate(
    freq_i = total_i / total_words,
    freq_we = total_we / total_words,
    ) 

head(i_we_df)
i_we_df <- i_we_df %>% 
  select(vlogId, freq_i, freq_we)
```

## Average word length

```{r}
all_words <- unique(words_df$word)

word_lengths <- tibble(word = all_words) %>%
  mutate(word_length = nchar(word))

avg_word_length_df <- words_df %>%
  left_join(word_lengths, by = "word") %>%
  group_by(vlogId) %>%
  summarise(avg_word_length = mean(word_length))

```


## Sentiment analysis

### Overall sentiment average and variance

```{r}
afinn_dict <- get_sentiments("afinn")

sentiment_df <- words_df %>%
  inner_join(afinn_dict, by = "word") %>%
  anti_join(stop_words, by = "word") %>% 
  group_by(vlogId) %>%
  summarize(sentiment_avg = mean(value),
            sentiment_var = var(value))

head(sentiment_df)
```

### Swear words

```{r}
# Download dictionary of swear words
download.file("https://www.freewebheaders.com/download/files/base-list-of-bad-words_text-file_2021_01_18.zip", "swear.zip")
unzip("swear.zip")

swear_words <- readLines("base-list-of-bad-words_text-file_2021_01_18.txt") %>%
  tibble(word = .) %>% 
  # Select only lines with a single word
  filter(!grepl("\\W+", word), word != "") 

swear_df <- words_df %>%
  semi_join(swear_words, by = "word") %>%
  count(vlogId) %>%
  rename(swear_words = n) %>%
  # Join with gender to add vloggs without swear words
  right_join(gender_df, by = "vlogId") %>% 
  replace_na(list(swear_words = 0)) %>%
  select(vlogId, swear_words)

head(swear_df)
```

# Merge all features into `vlogger_df`

- This is the list of features included in our models

```{r}
vlogger_df <- left_join(gender_df, pers_df, by = "vlogId") %>%
  left_join(audiovisual_df, by = "vlogId") %>%
  left_join(i_we_df, by = "vlogId") %>%
  left_join(avg_word_length_df, by = "vlogId") %>%
  left_join(sentiment_df, by = "vlogId") %>% 
  left_join(swear_df, by = "vlogId")

head(vlogger_df)
```

# Predictive model

- For now we are just using all features without interactions to predict all personality dimensions
- We plan on using a different approach for the final submission

```{r}
modeling_df <- vlogger_df %>%
  select(-vlogId)
model <- lm(cbind(Extr, Agr, Cons, Emot, Open) ~ ., data = modeling_df)
```

- Print R-squared
```{r}
sapply(summary(model), function(x) x$r.squared)
```

- Print significant variables
```{r}
getSigFeatures <- function(summ, alpha) {
  tibble(as.data.frame(summ$coefficients)) %>%
    mutate(feature = rownames(summ$coefficients)) %>% 
    filter(`Pr(>|t|)` < alpha) %>%
    select(feature)
}
sapply(summary(model), function(x) getSigFeatures(x, 0.01))
```

# Generating predictions

## The test set

```{r}
testset <- vlogger_df %>% 
  filter(is.na(Extr))
head(testset)
```

## Predictions

```{r}
predictions <- predict(model, newdata = testset)
head(predictions)
```


```{r}
# Compute output data frame
output <- testset %>% 
  mutate(
    Extr = predictions[,'Extr'], 
    Agr  = predictions[,'Agr' ],
    Cons = predictions[,'Cons'],
    Emot = predictions[,'Emot'],
    Open = predictions[,'Open']
  ) %>%
  select(vlogId, Extr:Open) %>% 
  # Convert to the specified output format
  gather(pers_axis, value, -vlogId) %>%
  arrange(vlogId, pers_axis) %>% 
  unite(Id, vlogId, pers_axis) %>%
  write_csv(file = "predictions.csv") 

head(output)
```
